group_by(location, device, filter) %>% # group by location, device ID, filter ID
mutate(
start_time = min(time), #start time for cartridge ID
latest_time = max(time), #end time for cartridge ID
total_run_time = as.numeric(difftime(latest_time, start_time, units = "hours")), #total run time for cartridge ID
elapsed_time = as.numeric(difftime(time, start_time, units = "hours")) #elapsed time of current row for cartridge ID
) %>%
mutate(interval = (elapsed_time %/% 24) + 1) %>% #create index for number of 24-hours periods (1-based)
ungroup()
all_daily <- all_df %>% #create stats by interval
group_by(location, device, filter, interval) %>% #for each unique 24-hour interval by device...
reframe(    start_time = min(time),
end_time = max(time),
n_readings = n(),
elapsed_time = difftime(end_time, start_time, units = c("hours")),
daily_mean_pm = mean(pm),
daily_95_pm = quantile(pm, .95),
daily_mean_rh = mean(rh),
daily_mean_t = mean(t))
all_daily <- all_daily %>%
mutate(
day = as.Date(start_time),
across(c('elapsed_time', 'daily_mean_pm', 'daily_95_pm', 'daily_mean_rh', 'daily_mean_t'), round, 1)) %>%
select(location, device, filter, day, elapsed_time, n_readings, daily_mean_pm, daily_95_pm, daily_mean_rh, daily_mean_t)%>%
arrange(desc(day))
#write daily tables
write.csv(recent_daily, "recent_daily.csv", row.names = FALSE) #for table
write.csv(all_daily, "all_daily.csv", row.names = FALSE)
#process all data from get_ceams_data
#This script is scheduled to run every 3 days on my task scheduler
rm(list = ls())
library(dplyr)
library(lubridate)
source("get_ceams_data.R")
# List of device IDs
device_ids <- c(
# Pretoria, South Africa
'AD00103', 'AD00104', 'AD00405',
# Taichung, Taiwan
'AD00094',
# Tainan, Taiwan
'AD00092','AD00101','AD00403','AD00406',
# Sonipat, India
'AD00096', 'AD00415', 'AD00416',
# Amity University Noida, India (88 starting in 2024)
'AD00088','AD00090','AD00413','AD00414',
# Amity University Manesar, India
'AD00408','AD00409',
# Adama, Ethiopia
'AD00410','AD00412',
#AASTU Addis Ababa, Ethiopia
'AD00091','AD00098','AD00093',
#JPL
'AD00402',
#Atlanta early November
'AD00417')
all_data <- get_ceams_data(device_ids) #call function in get_ceams_data.R
# Keep only the specified columns
all_data <- all_data %>%
select(cartridge_id, latitude, longitude, sn, timestamp_utc, pm25, fdpdp, volumetric_flowrate, pump_rh, pump_t)
all_data <- rename(all_data,
lat = latitude,
lng = longitude,
device = sn,
time = timestamp_utc,
filter = cartridge_id,
flow = volumetric_flowrate,
pressure = fdpdp,
rh = pump_rh,
t = pump_t,
pm = pm25)
all_data <- all_data %>%
filter(!is.na(time)) %>%
filter(pm > 0) %>%
filter(!is.na(pm)) %>%
filter(pm < quantile(pm, 0.9999))
all_data$time <- as.POSIXct(all_data$time, format= "%Y-%m-%dT%H:%M")
start_size = nrow(all_data) #size of all data pre-process
#Group location by device ID ------------------
pretoria <- all_data %>%
filter(device == 'AD00103' | device == 'AD00104' | device == 'AD00405' )%>%
mutate(location = 'Pretoria, South Africa')
pretoria_recent <- pretoria %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Pretoria, South Africa')
taichung <- all_data %>%
filter(device == 'AD00094')%>%
mutate(location = 'Taichung, Taiwan')
taichung_recent <- taichung %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Taichung, Taiwan')
tainan <- all_data %>%
filter(device == 'AD00092' | device == 'AD00101' | device == 'AD00403' | device == 'AD00406')%>%
mutate(location = 'Tainan, Taiwan')
tainan_recent <- tainan %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Tainan, Taiwan')
sonipat <- all_data %>%
filter(device == 'AD00096' | device == 'AD00415' | device == 'AD00416')%>%
mutate(location = 'Sonipat, India')
sonipat_recent <- sonipat %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Sonipat, India')
noida <- all_data %>%
filter(device == 'AD00088' | device == 'AD00090' | device == 'AD00413' | device == 'AD00414')%>%
mutate(location = 'Amity University Noida, India')
noida_recent <- noida %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Amity University Noida, India')
manesar <- all_data %>%
filter(device == 'AD00408' | device == 'AD00409')%>%
mutate(location = 'Amity University Manesar, India')
manesar_recent <- manesar %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Amity University Manesar, India')
adama <- all_data %>%
filter(device == 'AD00410' | device == 'AD00412')%>%
mutate(location = 'Adama, Ethiopia')
adama_recent <- adama %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Adama, Ethiopia')
aastu <- all_data %>%
filter(device == 'AD00091' | device == 'AD00098' | device == 'AD00093')%>%
mutate(location = 'AASTU, Ethiopia')
aastu_recent <- aastu %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'AASTU, Ethiopia')
jpl <- all_data %>%
filter(device == 'AD00402') %>%
mutate(location = 'JPL')
jpl_recent <- jpl %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'JPL')
atl <- all_data %>%
filter(device == 'AD00417') %>%
mutate(location = 'Atlanta')
atl_recent <- atl %>%
filter(time >= Sys.Date() - 14)%>%
mutate(location = 'Atlanta')
all_recent_df <- rbind(pretoria_recent, taichung_recent, tainan_recent, sonipat_recent, noida_recent, manesar_recent, adama_recent, aastu_recent, jpl_recent, atl_recent)
all_df <- rbind(pretoria, taichung, tainan, sonipat, noida, manesar, adama, aastu, jpl, atl)
#write all data tables
write.csv(all_recent_df, "all_recent_data.csv", row.names = FALSE) #for plot
write.csv(all_df, "all_data.csv", row.names = FALSE) #all data
#Run this if not pulling from get_ceams_data
#all_data <- read.csv("all_data.csv") #comment
#summary statistics:
#24-hour groups for recent data -----------------------------
all_recent_df <- all_recent_df %>%
group_by(location, device, filter) %>% # group by location, device ID, filter ID
mutate(
start_time = min(time), #start time for cartridge ID
latest_time = max(time), #end time for cartridge ID
total_run_time = as.numeric(difftime(latest_time, start_time, units = "hours")), #total run time for cartridge ID
elapsed_time = as.numeric(difftime(time, start_time, units = "hours")) #elapsed time of current row for cartridge ID
) %>%
mutate(interval = (elapsed_time %/% 24) + 1) %>% #create index for number of 24-hours periods (1-based)
ungroup()
recent_daily <- all_recent_df %>% #create stats by interval
group_by(location, device, filter, interval) %>% #for each unique 24-hour interval by device...
reframe(    start_time = min(time),
end_time = max(time),
n_readings = n(),
elapsed_time = difftime(end_time, start_time, units = c("hours")),
daily_mean_pm = mean(pm),
daily_95_pm = quantile(pm, .95),
daily_mean_rh = mean(rh),
daily_mean_t = mean(t))
recent_daily <- recent_daily %>%
mutate(
day = as.Date(start_time),
across(c('elapsed_time', 'daily_mean_pm', 'daily_95_pm', 'daily_mean_rh', 'daily_mean_t'), round, 1)) %>%
select(location, device, filter, day, elapsed_time, n_readings, daily_mean_pm, daily_95_pm, daily_mean_rh, daily_mean_t)%>%
arrange(desc(day))
#24-hour groups for ALL data -----------------------------
all_df <- all_df %>%
group_by(location, device, filter) %>% # group by location, device ID, filter ID
mutate(
start_time = min(time), #start time for cartridge ID
latest_time = max(time), #end time for cartridge ID
total_run_time = as.numeric(difftime(latest_time, start_time, units = "hours")), #total run time for cartridge ID
elapsed_time = as.numeric(difftime(time, start_time, units = "hours")) #elapsed time of current row for cartridge ID
) %>%
mutate(interval = (elapsed_time %/% 24) + 1) %>% #create index for number of 24-hours periods (1-based)
ungroup()
all_daily <- all_df %>% #create stats by interval
group_by(location, device, filter, interval) %>% #for each unique 24-hour interval by device...
reframe(    start_time = min(time),
end_time = max(time),
n_readings = n(),
elapsed_time = difftime(end_time, start_time, units = c("hours")),
daily_mean_pm = mean(pm),
daily_95_pm = quantile(pm, .95),
daily_mean_rh = mean(rh),
daily_mean_t = mean(t))
all_daily <- all_daily %>%
mutate(
day = as.Date(start_time),
across(c('elapsed_time', 'daily_mean_pm', 'daily_95_pm', 'daily_mean_rh', 'daily_mean_t'), round, 1)) %>%
select(location, device, filter, day, elapsed_time, n_readings, daily_mean_pm, daily_95_pm, daily_mean_rh, daily_mean_t)%>%
arrange(desc(day))
#write daily tables
write.csv(recent_daily, "recent_daily.csv", row.names = FALSE) #for table
write.csv(all_daily, "all_daily.csv", row.names = FALSE)
system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
gravi <- read_csv("maia_gravi.csv")
library(pacman)
p_load(tidyverse, openxlsx, here)
gravi <- read_csv("maia_gravi.csv")
system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
gravi <- read_csv("maia_gravi.csv")
rm(list = ls())
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
#download latest version of gravimetric data
json_content <- system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv", intern = TRUE)
library(jsonlite)
library(readr)
# Convert JSON response to a list
json_data <- fromJSON(paste(json_content, collapse = ""))
gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv
gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv
output <- system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv", intern = TRUE)
cat(output, sep = "\n")
output <- system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv", intern = TRUE)
json_data <- fromJSON(paste(output, collapse = ""))
download_url <- json_data$download_url
json_data <- fromJSON(paste(output, collapse = "\n"))
cat(output, sep = "\n")
cleaned_output <- gsub("[[:cntrl:]]", "", paste(output, collapse = "\n"))
cat(cleaned_output, sep = "\n")
json_data <- fromJSON(cleaned_output)
library(jsonlite)
output <- system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv", intern = TRUE)
json_data <- fromJSON(paste(output, collapse = "\n"))
# Run the GitHub API to get the file information
output <- system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv", intern = TRUE)
# Remove ANSI color codes from the output (color codes usually start with \033[ and end with m)
cleaned_output <- gsub("\033\\[[0-9;]*m", "", paste(output, collapse = "\n"))
# Check the cleaned output
cat(cleaned_output, sep = "\n")
# Try parsing the cleaned output as JSON
json_data <- fromJSON(cleaned_output)
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
gravi <- read_csv("maia_gravi.csv")
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
# system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
destfile <- here("maia_gravi.csv")  # File path relative to the project root
download.file(url, destfile, mode = "wb")
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
#
# system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
destfile <- here("maia_gravi.csv")  # File path relative to the project root
download.file(url, destfile, mode = "wb")
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
#
# system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
destfile <- here("maia_gravi.csv")  # File path relative to the project root
download.file(url, destfile, mode = "wb")
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
#
system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
destfile <- here("maia_gravi.csv")  # File path relative to the project root
download.file(url, destfile, mode = "wb")
gravi <- read_csv("maia_gravi.csv")
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
#
#system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
# url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
# destfile <- here("maia_gravi.csv")  # File path relative to the project root
# download.file(url, destfile, mode = "wb")
gravi <- read_csv("maia_gravi.csv")
initialrows = nrow(gravi) #number of rows at start: initialrows ~
### INITIAL CLEAN ######################
#remove unnecessary data
gravi <- gravi %>%
select(!c(error_flag, shutdown_mode, run_time_hrs:gravi_diff_ug)) %>%
rename(deploy_start = `deploy_s ar`) %>% #if there is a naming issue 11/18
mutate(deploy_start = as.POSIXct(deploy_start, format = "%m/%d/%Y %H:%M"))
View(gravi)
#process gravimetric data for Shiny app
rm(list = ls())
library(pacman)
p_load(tidyverse, openxlsx, here)
library(jsonlite)
## NEED TO FIX TRY HERE?
#
#system("gh api repos/LOrangeResearch/MAIA/contents/maia_gravi.csv > maia_gravi.csv")
# gravi <- read_csv("maia_gravi.csv")
# OLD METHOD - EXPIRES
# url <- "https://raw.githubusercontent.com/LOrangeResearch/MAIA/refs/heads/master/maia_gravi.csv?token=GHSAT0AAAAAAC735X7MEJNWTYNHLQUGUWPIZ6YO5UQ"
# destfile <- here("maia_gravi.csv")  # File path relative to the project root
# download.file(url, destfile, mode = "wb")
gravi <- read_csv("maia_gravi.csv")
initialrows = nrow(gravi) #number of rows at start: initialrows ~
### INITIAL CLEAN ######################
#remove unnecessary data
gravi <- gravi %>%
select(!c(error_flag, shutdown_mode, run_time_hrs:gravi_diff_ug)) %>%
rename(deploy_start = `deploy_start`) %>% #if there is a naming issue 11/18
mutate(deploy_start = as.POSIXct(deploy_start, format = "%m/%d/%Y %H:%M"))
# UNCLEANED DATA : gravi ~
#remove rows with empty gravimetric concentration
gravi_nonzero <- gravi %>%
filter(!is.na(gravi_con_ugm3))
rows_complete = nrow(gravi_nonzero)
emptyrows = initialrows - rows_complete #number of rows with NA for concentration: emptyrows ~
##### DATA CLEANING ########
#remove rows with not 24-hr run time or 2 L/min flowrate
gravi_valid <- gravi_nonzero %>%
filter(round(run_time_filter_hrs) > 18, # if runtime rounded to nearest whole <=18, delete row
round(flow_avg_l_min) == 2) # if flowrate rounded to nearest whole /= 2, delete row
badrows <- rows_complete - nrow(gravi_valid) #number of rows with bad runtime or flowrate: badrows ~
#remove outliers
gravi_valid1 <- gravi_valid %>%
filter(gravi_con_ugm3 <= 1000)
outliers <- anti_join(gravi_valid, gravi_valid1)
print(outliers) #outliers removed (values of outliers): outliers ~
outlier_num <- nrow(outliers) #number of outliers removed: outlier_num ~
###### ERROR COLUMN ###########
#make new variable; checkforerror; if gravi_con lower than 12 or filter pressure above 550, it's 1
gravi_new <- gravi_valid1 %>%
mutate(checkforerror = case_when(gravi_con_ugm3<=12~"1",
gravi_con_ugm3>12~"0",
#filter_dp>550~"1",
#filter_dp<=550~"0",
),
checkforerror = as.character(checkforerror),
checkforerror = as.numeric(checkforerror))
total_flag <- sum(gravi_new$checkforerror, na.rm = TRUE) #number of concentrations at or below 12 ug/m3: total_flag ~
#check for duplicate filter ID, none if 0
if (sum(duplicated(gravi_new$filter)) == 0) { #are there duplicate filters after cleaning? Y/N duplicates ~
duplicates <- "No"
} else {
duplicates <- "Yes"
}
#Removing dates out of PTA:
#convert to datetime
gravi_new <- gravi_new %>%
mutate(deploy_start = as.POSIXct(deploy_start))
alldates = nrow(gravi_new)
#remove early dates (earliest is May 2022)
gravi_new <- gravi_new %>%
filter(deploy_start >= as.POSIXct("2022-05-03"))
#dates removed before correct date
too_early = alldates - nrow(gravi_new)
#total rows removed
rows_removed <- nrow(gravi) - nrow(gravi_new)
#message for clean
cat("\n", "Starting size:", initialrows, "\n",
"Number of valid rows:", nrow(gravi_new), "\n",
"Total number of rows removed:", rows_removed, "\n",
"Rows with N/A for concentration:", emptyrows,"\n",
"Rows with runtime <18 h or flowrate not ~2 L/min:", badrows,"\n",
"Number of outliers:", outlier_num, "\n",
"Duplicate filters?", duplicates,"\n",
"Number of flagged runs for low concentration:", total_flag, "\n")
#check locations
gravi_new$lat <- round(gravi_new$lat, 2)
gravi_new$lng <- round(gravi_new$lng, 2)
gravi_new <- gravi_new %>%
mutate(Coordinates = paste(gravi_new$lat, gravi_new$lng, sep = ","))
print(unique(gravi_new$Coordinates))
# #GROUP BY LOCATION
#
# #     lat     lng  location
# #  33.69  -84.29  Atlanta - 37
# #  33.78  -84.35  ^^
# #  34.20 -118.17  Pasadena - 99
#
# # -25.90   28.11  South Africa - 83
# # -25.91   28.13  ^^
#
# #  28.54   77.34  Amity Noida, India - 19
#
# #  28.95   77.10  Sonipat, India - 9
#
# #  28.32   76.92   Gurugram, India - 6
#
# #  23.00  120.22  Tainan, Taiwan - 45
# #  22.99, 120.22  ^^
#
# #  24.16  120.62  Taichung, Taiwan - 5
#
# #  8.54   39.28    Adama, Ethiopia - 5
# #  8.99   38.71    Addis Ababa, Ethiopia - 3
atl <- gravi_new %>%
mutate(Location = "Atlanta") %>%
filter(lng == -84.29) #37
jpl <- gravi_new %>%
mutate(Location = "JPL") %>%
filter(lng == -118.17) # 99
sa <- gravi_new %>%
mutate(Location = "Pretoria, South Africa") %>%
filter(lng == 28.11 | lng == 28.13) #83
noida <- gravi_new %>% #19
mutate(Location = "Amity University Noida, India") %>%
filter(lng == 77.34)
guru <- gravi_new %>% #6
mutate(Location = "Amity University Manesar, India") %>%
filter(lng == 76.92)
sonipat <- gravi_new %>% #9
mutate(Location = "Sonipat, India") %>%
filter(lng == 77.10)
tainan <- gravi_new %>% #45
mutate(Location = "Tainan, Taiwan") %>%
filter(lng == 120.22)
taichung <- gravi_new %>% #5
mutate(Location = "Taichung, Taiwan") %>%
filter(lng == 120.62)
adama <- gravi_new %>% #5
mutate(Location = "Adama, Ethiopia") %>%
filter(lng == 39.28)
aa <- gravi_new %>% #3
mutate(Location = "AASTU, Ethiopia") %>%
filter(lng == 38.71)
#STATS: make final full dataset and summary dataset at the same time
# List of dataset variable names
variable_names <- c("atl", "sa", "noida", "guru", "sonipat", "tainan", "taichung", "adama", "aa", "jpl")
# Initialize an empty list to store results
results_list <- list()
all_data = data.frame()
# Loop through each dataset
for (var_name in variable_names) {
# Get the dataset from the variable name
dataset <- get(var_name)
all_data <- rbind(all_data, dataset)
# Compute number of points and mean for each unique entry in 'device'
summary_df <- dataset %>%
group_by(device) %>%
summarize(
count = n(),
mean_gravi_con_ugm3 = mean(gravi_con_ugm3, na.rm = TRUE),
med = quantile(gravi_con_ugm3, 0.5),
s5 = quantile(gravi_con_ugm3, 0.75),
n5 = quantile(gravi_con_ugm3, 0.95),
max = max(gravi_con_ugm3),
Location = Location
)
# Add a column to identify the dataset
summary_df <- summary_df %>%
mutate(dataset = var_name)
# Append the result to the list
results_list[[var_name]] <- summary_df
}
# Concatenate all results into one dataframe
summary_results <- do.call(rbind, results_list)
summary_results <- unique(summary_results)
summary_results <- summary_results %>% rename("Device" = device,
"Number of valid filters" = count,
"Mean PM2.5 Concentration (µg/m³)" = mean_gravi_con_ugm3,
"Median" = med,
"75th percentile" = s5,
"95th percentile" = n5,
"Max" = max)
summary_results <- summary_results %>% select(-dataset) #final
all_data <- all_data %>%
select(filter, device, deploy_start, gravi_con_ugm3, checkforerror, Location) %>%
rename ('Filter ID' = filter,
'Device' = device,
'PM2.5 Concentration (µg/m³)' = gravi_con_ugm3) %>%
mutate('Day' = as.Date(deploy_start)) %>%
select(-deploy_start, -checkforerror) #final
write.csv(all_data, "all_grav.csv", row.names = FALSE) #grav table
write.csv(summary_results, "grav_summary.csv", row.names = FALSE) #summary
shiny::runApp()
