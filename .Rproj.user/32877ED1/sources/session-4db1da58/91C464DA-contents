rm(list = ls())
library(tidyverse)
setwd('C:/Users/mcard/OneDrive/Documents/MAIA/CEAMS')
## SET-UP ######################################################
sa103 <- read.csv("south-africa-103.csv") # Sept 22 - Nov 23
sa104 <- read.csv("south-africa-104.csv") # Jan 24 - Mar 24
taichung94 <- read.csv("taichung-94.csv") # June 22 - Dec 23
tainan92 <- read.csv("tainan-92.csv")     # Aug 22 - May 23
tainan101 <- read.csv("tainan-101.csv")   # Aug 22 - May 23
tainan403 <- read.csv("tainan-403.csv")   # Oct 23 - onward
tainan406 <- read.csv("tainan-406.csv")   # Mar 10 - onward
delhi96 <- read.csv("delhi-96.csv")       # Aug 22 - Jan 23
noida102 <- read.csv("noida-102.csv")     # Jul 22 - Jan 23
noida90 <- read.csv("noida-90.csv")       # Jan 23 - Sep 23
noida409 <- read.csv("noida-409.csv")     # Nov 23 - Feb 24
noida88 <- read.csv("noida-88.csv")       # Mar 24 - onward
guru88 <- read.csv("gurugram-88.csv")     # Jul 22 - Nov 23
guru409 <- read.csv("gurugram-409.csv")   # no data
guru408 <- read.csv("gurugram-408.csv")   # Dec 23 - onward
aa91 <- read.csv("addis-ababa-91.csv")    # Jun 23 - Jul 23
aa93 <- read.csv("addis-ababa-93.csv")    # no data
aa98 <- read.csv("addis-ababa-98.csv")    # Apr 24 - onward
adama91 <- read.csv("adama-91.csv")       # Mar 24 - onward
e89 <- read.csv("emory-89.csv")   # Jan 22 - Apr 24
e95 <- read.csv("emory-95.csv")   # Jan 22 - May 22
j97 <- read.csv("jpl-97.csv")     # Nov 21 - Dec 21
j99 <- read.csv("jpl-99.csv")     # Aug 22 - Dec 22

## PART ONE: CLEAN ##################
cleaner <- function(data) {
  #data <- noida88 // use to test
  data <- rename(data,
                 device = dev_sn,
                 row = X, #row number for tracking
                 time = timestamp,
                 filter = cartridge_id,
                 vflow = volumetric_flowrate,
                 pm = plantower_pm25)
  data <- select(data, c(row, device, time, filter, vflow, pm))
  
  start_size = nrow(data)
  
  #convert time column from character to POSIXct 
  data$time <- as.POSIXct(data$time, format= "%Y-%m-%dT%H:%M")
  
  #remove zeros and high positive values
  dataclean <- data %>%
    filter(pm > 0) %>%
    filter(pm <= 1000) %>%
    filter(pm < quantile(pm, 0.9999))   #remove outliers some other way?
  outliers_removed = start_size - nrow(dataclean)
  
  #remove rows with not 2 L/min flow rate
  datagood <- dataclean %>%
    filter(round(vflow) == 2)
  flowrate_removed = nrow(dataclean) - nrow(datagood)
  
  #remove duplicate filter ID 
  datavalid <- datagood %>%
    # group by filter ID
    group_by(filter) %>%
    # check first datetime 
    mutate(start_time = min(time)) %>%
    filter(difftime(time, start_time, units = "hours") <= 25) %>%
    #now datavalid has no duplicate filter ID and only the first 25 hours of data for each filter
    mutate(
      latest_time = max(time),
      time_diff = difftime(latest_time, start_time, units = "hours")
    ) %>%
    ungroup()
  #remove less than 18 hours per filter 
  datavalid <- datavalid %>%
    filter(time_diff >= 18)
  
  #final number of valid sampling days
  N <- n_distinct(datavalid$filter)
  #number of rows removed for < 18 hours
  improper_runtime_days_removed = nrow(datagood) - nrow(datavalid)
  #Rows removed message  
  end_size = nrow(datavalid)
  rows_removed = start_size - end_size
  cat("Start size:", start_size, "End size:", end_size, "\n",
      "Total rows removed:", rows_removed, "\n", 
      "PM outlier rows removed:", outliers_removed, "\n",
      "Low flowrate rows removed:", flowrate_removed, "\n", 
      "Improper runtime rows removed:", improper_runtime_days_removed, "\n",
      "Number of distinct sampling days:", N, "\n")
  return(datavalid)
} #end cleaning function

#clean all at once ################
sa103_clean <- cleaner(sa103)
sa104_clean <- cleaner(sa104)
taichung94_clean <- cleaner(taichung94)
tainan92_clean <- cleaner(tainan92)
tainan101_clean <- cleaner(tainan101)
tainan403_clean <- cleaner(tainan403)
tainan406_clean <- cleaner(tainan406)
delhi96_clean <- cleaner(delhi96)
noida102_clean <- cleaner(noida102)
noida90_clean <- cleaner(noida90)
noida409_clean <- cleaner(noida409)
noida88_clean <- cleaner(noida88)
guru88_clean <- cleaner(guru88)
#guru409_clean <- cleaner(guru409)
guru408_clean <- cleaner(guru408)
aa91_clean <- cleaner(aa91)
#aa93_clean <- cleaner(aa93)
aa98_clean <- cleaner(aa98)
adama91_clean <- cleaner(adama91)
e89_clean <- cleaner(e89)
e95_clean <- cleaner(e95)
j97_clean <- cleaner(j97)
j99_clean <- cleaner(j99) 

## PART THREE: 24-HOUR AVERAGES ###############
daily <- function(datavalid) {
  #calculate 24-h statistics
  N <- n_distinct(datavalid$filter)
  
  datavalid <- datavalid %>%
    group_by(filter) %>%
    mutate(
      day = date(start_time),
      daily_mean_pm = mean(pm)) %>%
    ungroup()
  
  data_daily <- datavalid %>%
    select(device, filter, day, time_diff, daily_mean_pm) %>%
    distinct()
  
  cat("Number of distinct sampling days:", nrow(data_daily), "\n")
  return(data_daily)
} 
#end daily function

## get all daily averages at once ###################
s103d <- daily(sa103_clean)
s104d	<- daily(sa104_clean)
t94d <-	daily(taichung94_clean)
t92d <- daily(tainan92_clean)
t101d <- daily(tainan101_clean)
t403d	<- daily(tainan403_clean)
t406d	<- daily(tainan406_clean)
d96d	<- daily(delhi96_clean)
n102d <- daily(noida102_clean)
n90d	<- daily(noida90_clean)
n409d <- daily(noida409_clean)
g88d <- daily(guru88_clean) #noida 88 empty
g408d <- daily(guru408_clean)
aa91d <- daily(aa91_clean) #addis 91
aa98d <- daily(aa98_clean)
ad91d <- daily(adama91_clean) #adama 91
#
e89_daily <- daily(e89_clean)
e95_daily <- daily(e95_clean)
j97_daily <- daily(j97_clean)
j99_daily <- daily(j99_clean)
# day and daily_mean_pm can be used to plot 24-h averages

## CALENDAR ### 
#On the y axis would be unit at a specific site 
#so, if a unit sampled at > 1 site, it would get two rows

#Shade each point on the plot by its mean 24h PM2.5 concentration (Highest concentrations would be redder; lower concentrations bluer)

#Join all datasets above. either add y axis labels manually or use the method below. 
# need device name somewhow

dataset_list <- list(s103d = s103d, s104d = s104d, t94d = t94d, t92d = t92d, 
                     t101d = t101d, t403d = t403d, t406d = t406d, d96d = d96d, 
                     n102d = n102d, n90d = n90d, n409d = n409d, g88d = g88d, 
                     g408d = g408d, aa98d = aa98d, aa91d = aa91d, ad91d = ad91d)

combined_data <- bind_rows(lapply(names(dataset_list), function(name) {
  data <- dataset_list[[name]]
  data$device <- name
  data
}))

device_order <- c("s103d", "s104d", "t94d", "t92d", "t101d", "t403d", "t406d", "d96d",
                  "n102d", "n90d", "n409d", "g88d", "g408d", "aa98d", "aa91d", "ad91d")

# Convert the 'device' column to a factor with the specified order
combined_data$device <- factor(combined_data$device, levels = device_order)

# Add a new column to determine the color based on 'daily_mean_pm'
combined_data <- combined_data %>%
  mutate(color = case_when(
    daily_mean_pm > 140 ~ "red",
    daily_mean_pm >= 40 & daily_mean_pm <= 140 ~ "orange",
    TRUE ~ "black"  # default color
  ))

# PLOT ###
ggplot(combined_data, aes(x = day, y = device, color = color)) +
  geom_point(size = 2) +
  scale_y_discrete(
    breaks = device_order,
    labels = c("SA 103", "SA 104", "Taichung 94", "Tainan 92", "Tainan 101", "Tainan 403",
               "Tainan 406", "Delhi 96", "Noida 102", "Noida 90", "Noida 409", "Gurugram 88",
               "Gurugram 408", "Addis Ababa 98", "Addis Ababa 91", "Adama 91")) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "3 months", minor_breaks = "1 month") +
  scale_color_manual(
    name = "24-h mean PM2.5 concentration",
    values = c("black" = "black", "orange" = "orange", "red" = "red"),
    breaks = c("black", "orange", "red"),
    labels = c("Less than 40 µg/m³", "40-140 µg/m³", "Greater than 140 µg/m³")) + 
  labs(x = NULL, y = "Device", title = "Temporal Coverage of Device by Location") +
  theme_minimal() + 
  theme(legend.position = "top") 

  #end current version
